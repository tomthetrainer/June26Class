# Optimization Algorithms

# Table of Contents

1. Fundamentals of Machine Learning
2. Something else

<div style="page-break-after: always;"></div>

----------------------
# Stochastic Gradient Descent Learning

Mini Batch most widely used. 
 

-------------------
<div style="page-break-after: always;"></div>

----------------------
# Error Surface

Horizontal axis is weights of Neural Net

Vertical is error it makes

For SINGLE Linear Neuron with squared error this is always a quadratic bowl

Vertical Cross Sections are parabolas

Horizontal Cross Sections are elipses

For MultiLayer it is much more complex, but it is smooth and locally approximation as quadratic bowl is appropriate. 


-------------------
<div style="page-break-after: always;"></div>

----------------------
# Go Downhil to reduce error

* direction of steepest descent? 
  * Not so good 
  * tolerable but imperfect

-------------------
<div style="page-break-after: always;"></div>

----------------------
# Big Learning Rate

* Slosh to an fro across parabola

-------------------
<div style="page-break-after: always;"></div>

----------------------
# Optimization Algorithm

What we want to achieve, is that we go quickly along the ravine in directions that have small, but very consistent gradients. And we move slowly in directions with these big, but very inconsistent

* 

-------------------
<div style="page-break-after: always;"></div>

----------------------
# Stochastic Gradient Descent 

Choices
Full Batch
Less than full batch
Individual case == ONLINE

-------------------
<div style="page-break-after: always;"></div>

----------------------
# Mini Batch

* If dataset is highly redundant then Half Batch is a good as full
* Less computation
* Gradient for many cases leads to matrix-matrix multiplication 
* YEAH GPU's !!!
* Need to be balanced for classes
  * MMMMMM
  * FFFFFF
  * Bad
  *MFMFMFM = good
  
* SHUFFLE

-------------------
<div style="page-break-after: always;"></div>

----------------------
# Answer Use Mini Batch


* 

-------------------
<div style="page-break-after: always;"></div>

----------------------
# Starting steps

* Set Learning Rate
* Is error increasing or oscilating wildly? 
  * Reduce the rate
* Is the error decreasing slowly
  * Increase the learning Rate
  
** Not sure how to adjust Learning Rate in DL4J  

* Turn down the learning rate when the err

-------------------
<div style="page-break-after: always;"></div>





# A Diagram

![alt text](../resources/venn.png)





---------
<div style="page-break-after: always;"></div>

# Machine Learning Compared to Data Science/Mining

---------
<div style="page-break-after: always;"></div>
