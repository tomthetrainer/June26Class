# Data Ingestion Case Study: Text



# Table of Contents

* Text Representations


<div style="page-break-after: always;"></div>
----------------------
# Basic Concepts of Text Representation for Neural Networks

* PreProcessing and tokenization
* Bag of Words
* N-Grams
* Word2Vec
* Paragraph Vectors
* GloVE
* Words as Sequence of Characters





-------------------
<div style="page-break-after: always;"></div>

# PreProcessing and tokenization

* Tokenizer
	* Splits stream of words into individual words
	  * DefaultTokenizer
	  * NGramTokenizer 
  	  * PosUimaTokenizer
	  * UimaTokenizer
* PreProcessors
  * LowCasePreProcessor
  * StemmingPreprocessor
	  
 
 



-------------------
<div style="page-break-after: always;"></div>

# Bag of Words 

Corpus is represented as the bag(multiset) of its words. 

* No Grammar
* No order
* Frequency only

"Bob and Carol and Ted and Alice"

Becomes the List
["Bob","and","Carol","Ted","Alice"]

Term frequency
 [1,3,1,1,1]
 
 



-------------------
<div style="page-break-after: always;"></div>

# Bag of Words uses

* TfIDF
  * Frequency of word/document compared to word/corpus of documents




-------------------
<div style="page-break-after: always;"></div>

# Bag of Words Example 

* Lab Folder has example
* Tokenizer to read files from directory and label with filename
```
TokenizerFactory tokenizerFactory = new DefaultTokenizerFactory();

        LabelAwareIterator iterator = new FilenamesLabelAwareIterator.Builder()
                .addSourceFolder(new ClassPathResource("bow").getFile())
                .useAbsolutePathAsLabel(false)
                .build();
```				



-------------------
<div style="page-break-after: always;"></div>

# Bag of Words Example continued

* Code to show contents of iterator
```
while(iterator.hasNext()){
            LabelledDocument doc = iterator.nextDocument();
            System.out.println(doc.getContent());
            System.out.println(doc.getLabels().get(0));
        }

        iterator.reset();

```



-------------------
<div style="page-break-after: always;"></div>

# Bag of Words Example Continued



```
BagOfWordsVectorizer vectorizer = new BagOfWordsVectorizer.Builder()
                .setMinWordFrequency(1)
                .setStopWords(new ArrayList<String>())
                .setTokenizerFactory(tokenizerFactory)
                .setIterator(iterator)
                .build();
vectorizer.fit();
```				



-------------------
<div style="page-break-after: always;"></div>

# Bag of Words Example Continued

* Code to explore the contents of the Bag of Words

```
	log.info(vectorizer.getVocabCache().tokens().toString());
    System.out.println(vectorizer.getVocabCache().totalNumberOfDocs());
    System.out.println(vectorizer.getVocabCache().docAppearedIn("two."));
    System.out.println(vectorizer.getVocabCache().docAppearedIn("one."));
    System.out.println(vectorizer.getVocabCache().docAppearedIn("world"));
```



-------------------
<div style="page-break-after: always;"></div>

# NGrams 
* Contiguous sequence of n items from a sequence of text

Example "It is the year 2016"

Bi-grams "It is" "is the" "the year" "year 2016"
Tri-grams "It is the" "is the year" "the year 2016"

-------------------
<div style="page-break-after: always;"></div>

# NGram uses

* Provide more context than Bag of Words
* Used in some Neural Net for Speech Recognition to narrow the scope of prediction
  * RNN predicts next word out of top x percent of trigram for previous 2 word predictions
  

-------------------
<div style="page-break-after: always;"></div>

# NGram code Example

```

public static void main(String[] args) throws Exception{
        String toTokenize = "To boldly go where no one has gone before.";
        TokenizerFactory factory = new NGramTokenizerFactory(new DefaultTokenizerFactory(), 1, 2);
        Tokenizer tokenizer = factory.create(toTokenize);
        factory = new NGramTokenizerFactory(new DefaultTokenizerFactory(), 2, 3);
        List<String> tokens = factory.create(toTokenize).getTokens();
        log.info(tokens.toString());
```

Output

```
[To, boldly],  [boldly, go],  [go, where],......
[To, boldly, go],  [boldly, go, where] ......

```


-------------------
<div style="page-break-after: always;"></div>

# Word2Vec

* Model for word embeddings
* Vector Space 
* Each word in Corpus => Vector in Vector Space
* Relative location of word in vector space denotes relationship
	* Boy->Man Girl->Woman

-------------------
<div style="page-break-after: always;"></div>



# Word2Vec

* Model for word embeddings
* Vector Space 
* Each word in Corpus => Vector in Vector Space

-------------------
<div style="page-break-after: always;"></div>

# Word2Vec - Generating the Vector Space

* Neural Network trained to return word probabilities of a moving window
	* Given word "Paris", out of the corpus of words predict probility of each word occuring within say 5 words of the word "Paris"
* One hot Vector, size of every word in the corpus
* all 0's except for 1 representing the word
* See Demo https://ronxin.github.io/wevi/
* See example in intellij
* ALlows you to do word math
	* King - Man + Woman = (?) Queen
	


-------------------
<div style="page-break-after: always;"></div>

# One-hot encoding

* Vector, the size of the vocabulary, all 0's except for 1

![alt text](../resources/onehot.png)

-------------------
<div style="page-break-after: always;"></div>

# Two Methods for building word2vec

* CBOW
  * w1,w2,w4,w5 as input to Neural Net
	  * Context words 
  * Train net with w3 as target
	  * Focus word
* SKIP GRAM
  * Reverse of CBOW
  * Input is Focus word
  * Output is Context Words


 

-------------------
<div style="page-break-after: always;"></div>

# CBOW visually

![alt text](../resources/quickfox.png)

-------------------
<div style="page-break-after: always;"></div>

# CBOW visually

![alt text](../resources/quickfox1.png)

-------------------
<div style="page-break-after: always;"></div>

# CBOW visually

![alt text](../resources/quickfox2.png)

-------------------
<div style="page-break-after: always;"></div>

# CBOW visually

![alt text](../resources/word2VecCBOW.png)


-------------------
<div style="page-break-after: always;"></div>

# GloVE

* Vector Representation
* word-word co-occurence algorithm for generating vector

-------------------
<div style="page-break-after: always;"></div>



# Text as Sequence of Characters

Text can be treated as sequence of characters, and  neural network can be trained to answer the question. Given input character X predict the next character, and repeat.


-------------------
<div style="page-break-after: always;"></div>

# Recurrent Neural Networks and Sequence Data

* Recurrent Neural Networks have the capacity to recognize dependencies in time series data
* Breaking a text corpus into a series of single characters allows the network to learn dependencies such as the most common letter after a "Q" is a "U", when a quote has been opened it should eventually be closed.
* In the Lab you will train a neural network to write weather forecasts.


-------------------
<div style="page-break-after: always;"></div>

# Using Recurrent Neural Networks to write weather forecast

In this Lab you will write a RNN to train on Weather Forecasts. As it trains it will output a weather forecast. Watch as it learns words as sequences of characters along with sentence structure and more. 

Refer to your Lab Manual. 




-------------------
<div style="page-break-after: always;"></div>

#

-------------------
<div style="page-break-after: always;"></div>

#

-------------------
<div style="page-break-after: always;"></div>

# GloVe



-------------------
<div style="page-break-after: always;"></div>


