!SLIDE center subsection

# Introduction to Recurrent Neural Networks 

* Overview
* Benefits
* Modeling Sequences
* Details
* BackPropagation through time
* Tuning Guidelines
* PhysioNet Example


!SLIDE

# Table of Contents

* **&rArr;** Overview
* Benefits
* Modeling Sequences
* Details
* BackPropagation through time
* Tuning Guidelines
* PhysioNet Example

!SLIDE

# What is a Recurrent Neural Network?

* FeedForward Network with hidden state
* Hidden state with own internal dynamics
* Information can be stored in "hidden state" for a long time

***In this section when we refer to RNN we mean Graves LSTM as defined here https://arxiv.org/abs/1308.0850***

!SLIDE

# Long Short Term Memory RNNs Uses

* HandWriting Recognition
* Sequence/Time Series Data
* Sequence Generation
* Sequence Classification

!SLIDE

# Long Short Term Memory RNN's

# Steal pictures from dave

* Consider dynamic state of NN as Short term
* We want to make that last a long time(Improve upon Vanilla RNN)
* Create modules that allow information to
  * Gate in
  * Gate out when needed
  * In between gate is closed and information is preserved
  * Forget Gate

!SLIDE

# Long Short Term Memory RNN's

* Logistic write gate with each node in Recurrent Layer
* Write Gate
  * ON/OFF state determined by rest of the network
  * Write State=ON
	* State is saved
* Keep Gate 
  * On/OFF state determined by rest of Net
  * Keep Gate=ON
	* State is maintained
* Read Gate
  * On/Off State Determined by rest of network
  * Read Gate ON
	* Data in cell is output to network
* Data in the memory cell is actually analog
* It writes the data back to itself at each step while "keep" is closed using Weight of 1


!SLIDE

# Why RNN and Not deep FeedForward Network?

* Vanishing Gradient Challenge with Deep FeedForward Networks
* Makes training deep networks challenging
* Backward pass is Linear
  * Compound multiplication of values close to zero tend to vanish
	* Vanishing Gradient
  * Compound multiplication of large values tend to explode
* Forward pass is Non-linear
  * Activation functions (squashing functions) prevent activity vectors from exploding
* Solution is LSTM to preserve time based info isolated from backpropagation

<!-- Great picture from Hinton Coursera course Re-work -->
<!-- ![network diagram](../resources/vanishing_gradient.png) -->

!SLIDE

!SLIDE

* Overview
* **&rArr;** Benefits
* Modeling Sequences
* Details
* BackPropagation through time
* Tuning Guidelines
* PhysioNet Example

!SLIDE

# RNNs power

# Condense this

* Distributed Hidden State
* Multiple hidden units can be active at once
  * Can "remember" several different things
* Nonlinear
  * Allows updates to hidden state in complicated ways
* "With enough neurons and enough time they can compute anything that can be computed on your computer" (Hinton Lecture)


!SLIDE

# ARi's thoughts on content

* Condense key concepts
* When to use
* Why to use
* Additional considerations
  * BPTT
  * Activation considerations
  * Train Test split considerations
  


!SLIDE

# Another Benefit of RNN over FeedForward Network

* FeedForward network
  * One to one relationship input to ouptut
* Recurrent network
  * one to many
	  * One Image to many words in Caption
  * Many to many
	  * English to French
  * Many to one
	  * Voice classification

!SLIDE

# Benefit of Recurrent Neural Network Over FeedForward Network with Fixed Time Window

* Pre-configured window of time steps
  * Brittle hard coded
  * Requires domain knowledge of feature dependencies
* Flexible state information for flexible length events
  * Able to learn over long flexible event windows
  * Learns feature dependencies over learned flexible time window

!SLIDE



# What Types of Activity can an RNN model

* Oscillation
  * motor control, walking robots
* Sequences
  * Including long term dependencies
* Text as sequences of characters
* Text as Sequences of words

!SLIDE


# RNN Architectures

![alt text](../resources/RNN_architectures.png)

Add Captions somehow, or rebuild image

* Standard supervised learning
* Image Captioning
* Sentiment Analysis
* Video Captioning, Natural Language Translation
* Part of Speech Tagging
* Generative Mode for text

!SLIDE

# LSTM Recurrent Neural Networks Successes

* Anomaly detection
* Handwriting Recognition
* Speech Recogntion
* Image Captioning

!SLIDE

# Cursive handwriting recogntion

* Input is sequence of pen coordinates as text is written
* Output is sequence of characters
  * Graves & Schmidhuber (2009)
* If sample is not live sequence of small image samples as input works

!SLIDE

# Training Data Requirements for Natural Language Processing

* RNN's require much less training data than other solutions 

!SLIDE


!SLIDE

* Overview
* Benefits
* **&rArr;** Modeling Sequences
* Details
* BackPropagation through time
* Tuning Guidelines
* PhysioNet Example

!SLIDE

# Timeseries and Recurrent Networks

* When dealing with sequential or timeseries data
	* We prefer to apply Recurrent Networks
* Allows us to plug in how the data changes over time
	* Patient data collected periodically
	* State of power grid over time
	* Sequence of actions by customer

!SLIDE

# Recurrent Neural Networks and Sequence Data

# Move this up and shorten

* Recurrent Neural Networks have the capacity to recognize dependencies in time series data
* Breaking a text corpus into a series of single characters allows the network to learn dependencies such as the most common letter after a "Q" is a "U", when a quote has been opened it should eventually be closed
* In the Lab you will train a neural network to write weather forecasts

!SLIDE

# Differences between RNN and FeedForward Networks

# Add the picture

* RNN allows for modeling change in Vectors over time
* RNN takes Multiple sets of vectors and inputs
* FFN takes single input feature vector

!SLIDE


# Modeling Sequences


* Input Sequence to Output Sequence
  * French to Spanish
  * Speech Recognition
	* Sound Pressures to word identities
* Training Sequence try to predict the value for current step + 1

!SLIDE

# Sequence thinking outside the box

* Sequence of words make sense conceptually
* Sequence of network requests fit pattern as well
* Sequence of pixels in image?

!SLIDE


# Training Goal, one Sequence to another Sequence

# Group all these under examples section

* When modeling Sequential data we often want to turn one sequence into another sequence

* A phrase in english to a phrase in Spanish

* Sequence of audio ad convert into word identitites

!SLIDE

# Training Goal

* Next timestep of current sequence

!SLIDE


# Non Sequence data as Sequence data

* Pixels in an image , or Grid of pixels applied to next Grid
* works quite well, feels less natural

!SLIDE


# Supervised vs unsupervised

* Training to predict next term in sequence blurs the line between supervised and unsupervised

!SLIDE


# Patterns that may use the Long Term memory of RNN's

# Move to same examples

* Character Sequence
  * Parenthesis, quotes, brackets opened or closed
  * Relationship of period space Capitalization for beginning of sentence
* Oscilation
  * Normal
  * Abnormal
* Network activity patterns
  * Input packet followed by stream of output packets
  * Anomolies in that pattern
* Financial Transaction Sequences
  * Normal
  * Abnormal

!SLIDE

# Reorg of everything and condense

* Overview
* Benefits
* Modeling Sequences
* **&rArr;** Details
* BackPropagation through time
* Tuning Guidelines
* PhysioNet Example

!SLIDE

# How an LSTM RNN works

* LSTMs contain information outside the normal flow of the recurrent network

* Network learns to store data there, read data from there, replace data in there

!SLIDE

# How the gates function

* Gates block or pass on information based on its strength and import, which they filter with their own sets of weights


!SLIDE

# How the gates learn

* Gates learn when to allow data to enter, leave or be deleted through the iterative process of making guesses, backpropagating error, and adjusting weights via gradient descent

!SLIDE


!SLIDE

* Overview
* Benefits
* Modeling Sequences
* Details
* **&rArr;** BackPropagation through time
* Tuning Guidelines
* PhysioNet Example

!SLIDE


# RNN Updater:  Back Propagation through time

* How an RNN is updated
* Advanced topic
* The recurrent net is conceptually a layered net that re-uses the same weights
* Layered feed forward network with weights constrained at each layer to be the same



!SLIDE

# RNN Updater: Back Propagation through time...

* Compute the gradients as usual
* Modify to meet the constraint (time constraint previous slide)
* Represent RNN as feed forward net with shared weights
* Forward pass builds stack of activities at each time slice
* Backward pass peels activities off that stack and computes error derivatives
  * That is why called BackPropagation through time
* After back prop for each time step constrain weights to match


!SLIDE


# Learning Process Review

* Different sets of weights filter the input for input, output and forgetting 
* The forget gate is represented as a linear identity function, 
  * If the gate is open, the current state of the memory cell is simply multiplied by one, to propagate forward one more time step

!SLIDE


!SLIDE

# Table of Contents

* Overview
* Benefits
* Modeling Sequences
* Details
* BackPropagation through time
* **&rArr;**Tuning Guidelines
* PhysioNet Example

!SLIDE

# LSTM Hyperparameter Tuning

* Avoid Overfitting
  * Great performance on training Data
  * Bad performance on out-of-sample prediction
* Use Regularization helps: 
  * l1
  * l2
  * dropout
* Larger Network, more likely to overfit
  * Avoind trying to learn a million parameters from 10,000 examples
  * parameters > examples = trouble
  * More data is always better

!SLIDE

# LSTM Hyperparameter Tuning: Continued...

* Train over multiple epochs (complete passes through the dataset)
* Evaluate test set performance at each epoch to know when to stop (early stopping)
* The learning rate is the single most important hyperparameter 
  * Tune this using deeplearning4j-ui; see this graph
* In general, stacking layers can help
* For LSTMs, use the softsign (not softmax) activation function over tanh (it’s faster and less prone to saturation (~0 gradients))
* Updaters: RMSProp, AdaGrad or momentum (Nesterovs) are usually good choices. AdaGrad also decays the learning rate, which can help sometimes
* Finally, remember data normalization, MSE loss function + identity activation function for regression, Xavier weight initialization


!SLIDE

# Table of Contents

* Overview
* Benefits
* Modeling Sequences
* Details
* BackPropagation through time
* Tuning Guidelines
* **&rArr;** PhysioNet Example

!SLIDE



# Example: PhysioNet Raw Data

* Set-a
	* Directory of single files
	* One file per patient
	* 48 hours of ICU data
* Format
	* Header Line
	* 6 Descriptor Values at 00:00 
	* Collected at Admission 
	* 37 Irregularly sampled columns
	* Over 48 hours



!SLIDE

# Physionet Data

	Time,Parameter,Value
	00:00,RecordID,132601
	00:00,Age,74
	00:00,Gender,1
	00:00,Height,177.8
	00:00,ICUType,2
	00:00,Weight,75.9
	00:15,pH,7.39
	00:15,PaCO2,39
	00:15,PaO2,137
	00:56,pH,7.39
	00:56,PaCO2,37
	00:56,PaO2,222
	01:26,Urine,250
	01:26,Urine,635
	01:31,DiasABP,70
	01:31,FiO2,1
	01:31,HR,103
	01:31,MAP,94
	
!SLIDE

# Physionet Data Continued...
	
	
	01:31,MechVent,1
	01:31,SysABP,154
	01:34,HCT,24.9
	01:34,Platelets,115
	01:34,WBC,16.4
	01:41,DiasABP,52
	01:41,HR,102
	01:41,MAP,65
	01:41,SysABP,95
	01:56,DiasABP,64
	01:56,GCS,3
	01:56,HR,104
	01:56,MAP,85
	01:56,SysABP,132

!SLIDE

# Preparing Input Data

![alt text](../resources/preparing_input_data.png)


!SLIDE

# Preparing Input Data: continued


* Input was 3D Tensor (3d Matrix)
	* Mini-batch as first dimension
	* Feature Columns as second dimension
	* Timesteps as third dimension
* PhysioNet: Mini-batch size of 20, 43 columns, and 202 Timesteps
	* We have 173,720 values per Tensor input

TH- explain batch and minibatch in terms of training

!SLIDE

# Input Sequence


* A single training example gets the added dimension of timesteps per column

![alt text](../resources/physionet_input_table.png)

!SLIDE

![alt text](../resources/uneven_timesteps_and_masking.png)

!SLIDE

# Recurrent Networks For Classification

* This is the “many-to-one” setup
	* Traditionally we’d do hand coded feature extraction on timeseries and encode into a vector
		* Losing the time aspect to the data
	* The “many”-part allows us to input a sequence without losing the time domain aspect
* Input is a series of measurements aligned by timestep
	* 0,1,0,0
	* 1,0,1,1
* Output in this case is a classification
	* Example: “Fraud vs Normal transaction”

!SLIDE

# Sequence Classification with RNNs

* Recurrent Neural Networks have the ability to <i>model change of input over time</i>
* Older techniques (mostly) do not retain time domain
	* Hidden Markov Models do…
		* <i>but are more limited</i>
* Key Takeaway: 
	* <font color="red">For working with Timeseries data, RNNs will be more accurate</font>

!SLIDE



